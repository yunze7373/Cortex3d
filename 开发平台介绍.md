# GPU 开发平台统一架构与使用指南

> **这是一个「远程可访问 · 本地算力在日本 · 项目级 GPU 隔离 · 可长期扩展」的 GPU 加速开发与运行平台。**

---

## 快速连接

```bash
# Mac 连接 WSL
ssh han-wsl

# 进入项目
cd ~/projects/<项目名>

# 进入容器
docker compose exec <服务名> bash
```

---

## 一、系统架构

```
[客户端] Mac / iPhone
    ↓ SSH + ZeroTier (192.168.192.125)
[接入层] Windows 11 (NVIDIA Driver + OpenSSH)
    ↓ wsl -d Ubuntu
[运行层] WSL2 Ubuntu (Docker + NVIDIA Container Toolkit)
    ↓
[项目层] Docker Containers (每项目独立 compose, GPU 直通)
```

**核心原则：**
- Windows = 门面 + 驱动（不跑项目）
- WSL = 项目管理（不 pip install）
- Docker = 项目运行环境（完全隔离）

---

## 二、目录规范

```
~/projects/
├─ cortex3d/          # 3D 模型生成
├─ llm-api/           # LLM 推理
├─ train-xxx/         # 训练项目
└─ templates/gpu-base # GPU 模板

~/data/
├─ datasets/          # 共享数据集
├─ models/            # 模型权重
└─ outputs/           # 输出结果
```

---

## 三、新建 GPU 项目标准流程

### Step 1: 创建 Dockerfile

```dockerfile
FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
# 根据 GPU 设置架构 (RTX 3080 Ti = 8.6, RTX 4090 = 8.9)
ENV TORCH_CUDA_ARCH_LIST="8.6"

# 基础依赖
RUN apt-get update && apt-get install -y \
    python3 python3-pip python3-dev git \
    libgl1-mesa-glx libglib2.0-0 \
    ninja-build g++ \
    && rm -rf /var/lib/apt/lists/*

# PyTorch (根据 CUDA 版本选择)
RUN pip3 install --no-cache-dir \
    torch torchvision \
    --index-url https://download.pytorch.org/whl/cu121

# 项目依赖 (根据需要添加)
# RUN pip3 install --no-cache-dir ...

# ⚠️ 需要编译的包 (如 nvdiffrast) 必须加验证
# RUN pip3 install --no-cache-dir --no-build-isolation \
#     git+https://github.com/xxx/xxx/ \
#     && python3 -c "import xxx; print('✅ xxx OK')"

WORKDIR /workspace

# 最终验证 (构建时确认依赖正确)
RUN python3 -c "import torch; print(f'✅ PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

### Step 2: 创建 compose.yml

```yaml
services:
  dev:
    build:
      context: .
      dockerfile: Dockerfile
    image: <项目名>:latest
    container_name: <项目名>-dev
    working_dir: /workspace
    volumes:
      - ./:/workspace
      - hf-cache:/root/.cache/huggingface
      - ~/data:/data
    shm_size: "8gb"
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  hf-cache:
```

### Step 3: 构建并运行

```bash
# 首次构建
docker compose build

# 启动
docker compose up -d

# 进入容器
docker compose exec dev bash

# 验证 GPU
python3 -c "import torch; print(torch.cuda.get_device_name(0))"
```

---

## 四、常见问题与解决方案

### ❌ ModuleNotFoundError: No module named 'xxx'

**原因：** 依赖没写进 Dockerfile，或构建时静默失败

**解决：**
1. 确保依赖在 Dockerfile 中
2. 添加安装后验证步骤
3. 重新构建 `docker compose build --no-cache`

```dockerfile
# 正确写法：安装 + 验证
RUN pip3 install xxx \
    && python3 -c "import xxx; print('✅ xxx installed')"
```

### ❌ nvdiffrast / PyTorch3D 编译失败

**原因：** 需要 CUDA 架构信息，但 Docker 构建时没有 GPU

**解决：** 设置 `TORCH_CUDA_ARCH_LIST` 环境变量

```dockerfile
# RTX 3080 Ti
ENV TORCH_CUDA_ARCH_LIST="8.6"

# RTX 4090
ENV TORCH_CUDA_ARCH_LIST="8.9"

# 多 GPU 兼容
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9"
```

### ❌ 容器重建后依赖丢失

**原因：** 手动在容器里 pip install，没写进 Dockerfile

**解决：** 
- ⚠️ **永远不要**在容器里手动 pip install 后就认为"装好了"
- 所有依赖必须写进 Dockerfile，然后 `docker compose build`

### ❌ SSH 连接断开后任务中断

**解决：** 使用 tmux

```bash
# 创建会话
tmux new -s train

# 断线后恢复
tmux attach -t train
```

---

## 五、远程连接方式

### macOS

```bash
ssh han-wsl
cd ~/projects/<项目名>
docker compose exec dev bash
```

### iPhone (Termius)

- Host: `192.168.192.125`
- User: `han`
- Key: SSH 私钥

---

## 六、多项目管理规则

| 规则 | 目的 |
|------|------|
| 每项目一个 compose.yml | 避免端口/依赖冲突 |
| 每项目独立镜像名 | 防止误用 |
| 公共数据走 `~/data` | 不重复下载 |
| 不共用 Python 环境 | 彻底隔离 |

---

## 七、GPU 架构速查表

| GPU | 架构 | TORCH_CUDA_ARCH_LIST |
|-----|------|----------------------|
| RTX 3060/3070/3080/3090 | Ampere | 8.6 |
| RTX 3080 Ti | Ampere | 8.6 |
| RTX 4060/4070/4080/4090 | Ada Lovelace | 8.9 |
| A100 | Ampere | 8.0 |
| H100 | Hopper | 9.0 |

---

## 八、核心思想

> **GPU 是稀缺资源，Docker 是隔离边界，WSL 是稳定底座，Windows 只是入口。**

- ✅ 所有依赖写进 Dockerfile
- ✅ 安装后立即验证
- ✅ 使用 `--no-cache` 排查问题
- ✅ tmux 保护长时间任务
