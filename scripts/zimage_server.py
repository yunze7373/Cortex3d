#!/usr/bin/env python3
"""
Z-Image-Turbo æœ¬åœ°æ¨ç†æœåŠ¡
æä¾› REST API ç”¨äºå›¾åƒç”Ÿæˆ

æ¨¡å‹: Tongyi-MAI/Z-Image-Turbo (6Bå‚æ•°)
- 8æ­¥æ¨ç†ï¼Œäºšç§’çº§å»¶è¿Ÿ
- æ”¯æŒä¸­è‹±æ–‡åŒè¯­
- ä¼˜ç§€çš„æ–‡å­—æ¸²æŸ“èƒ½åŠ›
- 16GB VRAM å³å¯è¿è¡Œ

API ç«¯ç‚¹:
- GET  /health   - å¥åº·æ£€æŸ¥
- POST /generate - æ–‡ç”Ÿå›¾ (Text-to-Image)
- POST /img2img  - å›¾ç”Ÿå›¾ (Image-to-Image)
- GET  /info     - æ¨¡å‹ä¿¡æ¯
"""

import os
import sys
import base64
import time
import torch
from io import BytesIO
from datetime import datetime
from flask import Flask, request, jsonify

app = Flask(__name__)

# å…¨å±€ Pipeline
pipe = None
model_loaded = False


def load_model():
    """åŠ è½½ Z-Image-Turbo æ¨¡å‹"""
    global pipe, model_loaded
    
    print("=" * 60)
    print("ğŸš€ Z-Image-Turbo æœ¬åœ°æ¨ç†æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ PyTorch: {torch.__version__}")
    print(f"ğŸ® CUDA: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print("")
    
    print("ğŸ“¦ æ­£åœ¨åŠ è½½ Z-Image-Turbo æ¨¡å‹...")
    print("   æ¥æº: Tongyi-MAI/Z-Image-Turbo (HuggingFace)")
    print("   å‚æ•°: 6B")
    print("   ç²¾åº¦: bfloat16")
    
    start_time = time.time()
    
    try:
        from diffusers import ZImagePipeline
        
        pipe = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )
        
        # å°è¯•ç›´æ¥åŠ è½½åˆ° GPU
        use_cpu_offload = False
        try:
            # å…ˆæ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            pipe.to("cuda")
            print("   âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU")
        except (RuntimeError, torch.cuda.OutOfMemoryError) as oom_e:
            print(f"   âš ï¸  GPU æ˜¾å­˜ä¸è¶³: {oom_e}")
            print("   ğŸ”„ å¯ç”¨ Sequential CPU Offload æ¨¡å¼...")
            torch.cuda.empty_cache()
            # é‡æ–°åŠ è½½å¹¶ä½¿ç”¨ CPU offload
            del pipe
            torch.cuda.empty_cache()
            pipe = ZImagePipeline.from_pretrained(
                "Tongyi-MAI/Z-Image-Turbo",
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
            )
            pipe.enable_sequential_cpu_offload()
            use_cpu_offload = True
            print("   âœ… CPU Offload æ¨¡å¼å·²å¯ç”¨ (æ¨ç†é€Ÿåº¦ä¼šç•¥æ…¢)")
        
        # å°è¯•å¯ç”¨ Flash Attention
        if not use_cpu_offload:
            try:
                pipe.transformer.set_attention_backend("flash")
                print("   âœ… Flash Attention 2 å·²å¯ç”¨")
            except Exception as e:
                print(f"   âš ï¸  Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨ SDPA: {e}")
        
        # å¯é€‰: ç¼–è¯‘æ¨¡å‹ (é¦–æ¬¡æ¨ç†ä¼šæ…¢ï¼Œä¹‹åæ›´å¿«)
        # pipe.transformer.compile()
        
        load_time = time.time() - start_time
        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.1f}ç§’")
        if use_cpu_offload:
            print("   ğŸ“Œ è¿è¡Œæ¨¡å¼: CPU Offload (èŠ‚çœæ˜¾å­˜)")
        print(f"ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:8199")
        print("=" * 60)
        
        model_loaded = True
        
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "ok" if model_loaded else "loading",
        "model": "Z-Image-Turbo",
        "cuda": torch.cuda.is_available(),
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
    })


@app.route("/generate", methods=["POST"])
def generate():
    """
    ç”Ÿæˆå›¾åƒ
    
    POST JSON:
    {
        "prompt": "æè¿°æ–‡æœ¬ (æ”¯æŒä¸­è‹±æ–‡)",
        "width": 1024,      // é»˜è®¤ 1024
        "height": 1024,     // é»˜è®¤ 1024
        "steps": 9,         // é»˜è®¤ 9 (å®é™… 8 æ¬¡ DiT å‰å‘)
        "seed": 42,         // å¯é€‰ï¼Œéšæœºç§å­
        "negative_prompt": "..."  // å¯é€‰ï¼Œè´Ÿé¢æç¤ºè¯ (Turboæ¨¡å‹æ•ˆæœæœ‰é™)
    }
    
    è¿”å›:
    {
        "image": "base64ç¼–ç çš„PNGå›¾åƒ",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "time": 1.23
    }
    """
    if not model_loaded:
        return jsonify({"error": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•"}), 503
    
    try:
        data = request.json or {}
        
        prompt = data.get("prompt", "")
        width = data.get("width", 1024)
        height = data.get("height", 1024)
        steps = data.get("steps", 9)  # Z-Image-Turbo æ¨è 9 æ­¥
        seed = data.get("seed", None)
        
        if not prompt:
            return jsonify({"error": "prompt å‚æ•°æ˜¯å¿…éœ€çš„"}), 400
        
        # éªŒè¯å°ºå¯¸
        if width < 256 or width > 2048 or height < 256 or height > 2048:
            return jsonify({"error": "å°ºå¯¸èŒƒå›´: 256-2048"}), 400
        
        # ç”Ÿæˆå™¨ (ç§å­)
        if seed is None:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.Generator("cuda").manual_seed(seed)
        
        print(f"\nğŸ¨ [{datetime.now().strftime('%H:%M:%S')}] ç”Ÿæˆè¯·æ±‚")
        print(f"   Prompt: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
        print(f"   å°ºå¯¸: {width}x{height}, æ­¥æ•°: {steps}, ç§å­: {seed}")
        
        start_time = time.time()
        
        # ç”Ÿæˆå›¾åƒ
        image = pipe(
            prompt=prompt,
            height=height,
            width=width,
            num_inference_steps=steps,
            guidance_scale=0.0,  # Turbo æ¨¡å‹ä¸éœ€è¦ guidance
            generator=generator,
        ).images[0]
        
        gen_time = time.time() - start_time
        
        # è½¬ base64
        buffer = BytesIO()
        image.save(buffer, format="PNG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {gen_time:.2f}ç§’")
        
        return jsonify({
            "image": img_b64,
            "width": width,
            "height": height,
            "seed": seed,
            "time": round(gen_time, 2),
        })
        
    except torch.cuda.OutOfMemoryError:
        print("   âŒ CUDA å†…å­˜ä¸è¶³!")
        torch.cuda.empty_cache()
        return jsonify({"error": "GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¾ƒå°çš„å°ºå¯¸"}), 507
        
    except Exception as e:
        print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route("/info", methods=["GET"])
def info():
    """æ¨¡å‹ä¿¡æ¯"""
    return jsonify({
        "model": "Z-Image-Turbo",
        "developer": "Tongyi-MAI (é˜¿é‡Œå·´å·´é€šä¹‰)",
        "parameters": "6B",
        "recommended_steps": 9,
        "supported_resolutions": [512, 768, 1024, 1280, 1536, 2048],
        "features": [
            "ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ",
            "ä¼˜ç§€çš„æ–‡å­—æ¸²æŸ“",
            "ç…§ç‰‡çº§çœŸå®æ„Ÿ",
            "8æ­¥å¿«é€Ÿæ¨ç†",
            "å›¾ç”Ÿå›¾ (img2img)",
        ],
        "endpoints": [
            {"method": "GET", "path": "/health", "description": "å¥åº·æ£€æŸ¥"},
            {"method": "POST", "path": "/generate", "description": "æ–‡ç”Ÿå›¾"},
            {"method": "POST", "path": "/img2img", "description": "å›¾ç”Ÿå›¾"},
        ],
        "gpu": {
            "name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "vram_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1) if torch.cuda.is_available() else None,
        }
    })


@app.route("/img2img", methods=["POST"])
def img2img():
    """
    å›¾ç”Ÿå›¾ (Image-to-Image)
    
    ä½¿ç”¨ SDEdit æ–¹å¼ï¼šå¯¹è¾“å…¥å›¾åƒæ·»åŠ å™ªå£°ç„¶åå»å™ªï¼Œå®ç°é£æ ¼å˜æ¢æˆ–å†…å®¹ä¿®æ”¹ã€‚
    
    POST JSON:
    {
        "prompt": "æè¿°æ–‡æœ¬ (æ”¯æŒä¸­è‹±æ–‡)",
        "image": "base64ç¼–ç çš„è¾“å…¥å›¾åƒ",
        "strength": 0.75,   // 0.0-1.0, è¶Šé«˜å˜åŒ–è¶Šå¤§
        "width": 1024,      // å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åŸå›¾å°ºå¯¸
        "height": 1024,     // å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åŸå›¾å°ºå¯¸
        "steps": 9,         // é»˜è®¤ 9
        "seed": 42          // å¯é€‰ï¼Œéšæœºç§å­
    }
    
    è¿”å›:
    {
        "image": "base64ç¼–ç çš„PNGå›¾åƒ",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "strength": 0.75,
        "time": 1.23
    }
    """
    if not model_loaded:
        return jsonify({"error": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•"}), 503
    
    try:
        from PIL import Image
        import numpy as np
        
        data = request.json or {}
        
        prompt = data.get("prompt", "")
        image_b64 = data.get("image", "")
        strength = float(data.get("strength", 0.75))
        steps = data.get("steps", 9)
        seed = data.get("seed", None)
        
        if not prompt:
            return jsonify({"error": "prompt å‚æ•°æ˜¯å¿…éœ€çš„"}), 400
        
        if not image_b64:
            return jsonify({"error": "image å‚æ•°æ˜¯å¿…éœ€çš„ (base64ç¼–ç )"}), 400
        
        # éªŒè¯ strength
        if strength < 0.0 or strength > 1.0:
            return jsonify({"error": "strength èŒƒå›´: 0.0-1.0"}), 400
        
        # è§£ç è¾“å…¥å›¾åƒ
        try:
            # å¤„ç†å¯èƒ½çš„ data:image/png;base64, å‰ç¼€
            if "base64," in image_b64:
                image_b64 = image_b64.split("base64,")[1]
            image_data = base64.b64decode(image_b64)
            init_image = Image.open(BytesIO(image_data)).convert("RGB")
        except Exception as e:
            return jsonify({"error": f"å›¾åƒè§£ç å¤±è´¥: {e}"}), 400
        
        # è·å–/è®¾ç½®å°ºå¯¸
        width = data.get("width", init_image.width)
        height = data.get("height", init_image.height)
        
        # éªŒè¯å°ºå¯¸
        if width < 256 or width > 2048 or height < 256 or height > 2048:
            return jsonify({"error": "å°ºå¯¸èŒƒå›´: 256-2048"}), 400
        
        # è°ƒæ•´è¾“å…¥å›¾åƒå°ºå¯¸
        if init_image.width != width or init_image.height != height:
            init_image = init_image.resize((width, height), Image.Resampling.LANCZOS)
        
        # ç”Ÿæˆå™¨ (ç§å­)
        if seed is None:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.Generator("cuda").manual_seed(seed)
        
        print(f"\nğŸ–¼ï¸ [{datetime.now().strftime('%H:%M:%S')}] å›¾ç”Ÿå›¾è¯·æ±‚")
        print(f"   Prompt: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
        print(f"   å°ºå¯¸: {width}x{height}, å¼ºåº¦: {strength}, æ­¥æ•°: {steps}, ç§å­: {seed}")
        
        start_time = time.time()
        
        # SDEdit: è®¡ç®—å®é™…æ­¥æ•°
        # strength=0.75 æ„å‘³ç€è·³è¿‡ 25% çš„å»å™ªæ­¥éª¤
        actual_steps = int(steps * strength)
        if actual_steps < 1:
            actual_steps = 1
        
        # ä½¿ç”¨ VAE ç¼–ç è¾“å…¥å›¾åƒåˆ°æ½œç©ºé—´
        init_image_tensor = pipe.image_processor.preprocess(init_image)
        init_image_tensor = init_image_tensor.to(device=pipe.device, dtype=pipe.dtype)
        
        # ç¼–ç åˆ°æ½œç©ºé—´
        latents = pipe.vae.encode(init_image_tensor).latent_dist.sample(generator)
        latents = latents * pipe.vae.config.scaling_factor
        
        # è®¾ç½®è°ƒåº¦å™¨
        pipe.scheduler.set_timesteps(steps)
        
        # è®¡ç®—å¼€å§‹çš„æ—¶é—´æ­¥
        start_step = int(steps * (1 - strength))
        timesteps = pipe.scheduler.timesteps[start_step:]
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn(latents.shape, generator=generator, device=pipe.device, dtype=pipe.dtype)
        latents = pipe.scheduler.add_noise(latents, noise, timesteps[:1])
        
        # ç¼–ç æç¤ºè¯
        prompt_embeds, pooled_prompt_embeds = pipe.encode_prompt(
            prompt=prompt,
            device=pipe.device,
            num_images_per_prompt=1,
            do_classifier_free_guidance=False,
        )
        
        # å»å™ªå¾ªç¯
        for i, t in enumerate(timesteps):
            # é¢„æµ‹å™ªå£°
            noise_pred = pipe.transformer(
                hidden_states=latents,
                timestep=t.unsqueeze(0),
                encoder_hidden_states=prompt_embeds,
                pooled_projections=pooled_prompt_embeds,
                return_dict=False,
            )[0]
            
            # æ›´æ–°æ½œå˜é‡
            latents = pipe.scheduler.step(noise_pred, t, latents, return_dict=False)[0]
        
        # è§£ç å›¾åƒ
        latents = latents / pipe.vae.config.scaling_factor
        image = pipe.vae.decode(latents, return_dict=False)[0]
        image = pipe.image_processor.postprocess(image, output_type="pil")[0]
        
        gen_time = time.time() - start_time
        
        # è½¬ base64
        buffer = BytesIO()
        image.save(buffer, format="PNG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {gen_time:.2f}ç§’")
        
        return jsonify({
            "image": img_b64,
            "width": width,
            "height": height,
            "seed": seed,
            "strength": strength,
            "time": round(gen_time, 2),
        })
        
    except torch.cuda.OutOfMemoryError:
        print("   âŒ CUDA å†…å­˜ä¸è¶³!")
        torch.cuda.empty_cache()
        return jsonify({"error": "GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¾ƒå°çš„å°ºå¯¸"}), 507
        
    except Exception as e:
        print(f"   âŒ å›¾ç”Ÿå›¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    load_model()
    
    # å¯åŠ¨æœåŠ¡
    # æ³¨æ„: ä½¿ç”¨ threaded=False é¿å…å¤šçº¿ç¨‹è®¿é—® CUDA
    app.run(
        host="0.0.0.0",
        port=8199,
        threaded=False,
        debug=False
    )
