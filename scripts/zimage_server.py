#!/usr/bin/env python3
"""
Z-Image-Turbo æœ¬åœ°æ¨ç†æœåŠ¡
æä¾› REST API ç”¨äºå›¾åƒç”Ÿæˆ

æ¨¡å‹: Tongyi-MAI/Z-Image-Turbo (6Bå‚æ•°)
- 8æ­¥æ¨ç†ï¼Œäºšç§’çº§å»¶è¿Ÿ
- æ”¯æŒä¸­è‹±æ–‡åŒè¯­
- ä¼˜ç§€çš„æ–‡å­—æ¸²æŸ“èƒ½åŠ›
- 16GB VRAM å³å¯è¿è¡Œ

API ç«¯ç‚¹:
- GET  /health  - å¥åº·æ£€æŸ¥
- POST /generate - ç”Ÿæˆå›¾åƒ
"""

import os
import sys
import base64
import time
import torch
from io import BytesIO
from datetime import datetime
from flask import Flask, request, jsonify

app = Flask(__name__)

# å…¨å±€ Pipeline
pipe = None
model_loaded = False


def load_model():
    """åŠ è½½ Z-Image-Turbo æ¨¡å‹"""
    global pipe, model_loaded
    
    print("=" * 60)
    print("ğŸš€ Z-Image-Turbo æœ¬åœ°æ¨ç†æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ PyTorch: {torch.__version__}")
    print(f"ğŸ® CUDA: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print("")
    
    print("ğŸ“¦ æ­£åœ¨åŠ è½½ Z-Image-Turbo æ¨¡å‹...")
    print("   æ¥æº: Tongyi-MAI/Z-Image-Turbo (HuggingFace)")
    print("   å‚æ•°: 6B")
    print("   ç²¾åº¦: bfloat16")
    
    start_time = time.time()
    
    try:
        from diffusers import ZImagePipeline
        
        pipe = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )
        pipe.to("cuda")
        
        # å°è¯•å¯ç”¨ Flash Attention
        try:
            pipe.transformer.set_attention_backend("flash")
            print("   âœ… Flash Attention 2 å·²å¯ç”¨")
        except Exception as e:
            print(f"   âš ï¸  Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨ SDPA: {e}")
        
        # å¯é€‰: ç¼–è¯‘æ¨¡å‹ (é¦–æ¬¡æ¨ç†ä¼šæ…¢ï¼Œä¹‹åæ›´å¿«)
        # pipe.transformer.compile()
        
        load_time = time.time() - start_time
        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.1f}ç§’")
        print(f"ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:8199")
        print("=" * 60)
        
        model_loaded = True
        
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "ok" if model_loaded else "loading",
        "model": "Z-Image-Turbo",
        "cuda": torch.cuda.is_available(),
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
    })


@app.route("/generate", methods=["POST"])
def generate():
    """
    ç”Ÿæˆå›¾åƒ
    
    POST JSON:
    {
        "prompt": "æè¿°æ–‡æœ¬ (æ”¯æŒä¸­è‹±æ–‡)",
        "width": 1024,      // é»˜è®¤ 1024
        "height": 1024,     // é»˜è®¤ 1024
        "steps": 9,         // é»˜è®¤ 9 (å®é™… 8 æ¬¡ DiT å‰å‘)
        "seed": 42,         // å¯é€‰ï¼Œéšæœºç§å­
        "negative_prompt": "..."  // å¯é€‰ï¼Œè´Ÿé¢æç¤ºè¯ (Turboæ¨¡å‹æ•ˆæœæœ‰é™)
    }
    
    è¿”å›:
    {
        "image": "base64ç¼–ç çš„PNGå›¾åƒ",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "time": 1.23
    }
    """
    if not model_loaded:
        return jsonify({"error": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•"}), 503
    
    try:
        data = request.json or {}
        
        prompt = data.get("prompt", "")
        width = data.get("width", 1024)
        height = data.get("height", 1024)
        steps = data.get("steps", 9)  # Z-Image-Turbo æ¨è 9 æ­¥
        seed = data.get("seed", None)
        
        if not prompt:
            return jsonify({"error": "prompt å‚æ•°æ˜¯å¿…éœ€çš„"}), 400
        
        # éªŒè¯å°ºå¯¸
        if width < 256 or width > 2048 or height < 256 or height > 2048:
            return jsonify({"error": "å°ºå¯¸èŒƒå›´: 256-2048"}), 400
        
        # ç”Ÿæˆå™¨ (ç§å­)
        if seed is None:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.Generator("cuda").manual_seed(seed)
        
        print(f"\nğŸ¨ [{datetime.now().strftime('%H:%M:%S')}] ç”Ÿæˆè¯·æ±‚")
        print(f"   Prompt: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
        print(f"   å°ºå¯¸: {width}x{height}, æ­¥æ•°: {steps}, ç§å­: {seed}")
        
        start_time = time.time()
        
        # ç”Ÿæˆå›¾åƒ
        image = pipe(
            prompt=prompt,
            height=height,
            width=width,
            num_inference_steps=steps,
            guidance_scale=0.0,  # Turbo æ¨¡å‹ä¸éœ€è¦ guidance
            generator=generator,
        ).images[0]
        
        gen_time = time.time() - start_time
        
        # è½¬ base64
        buffer = BytesIO()
        image.save(buffer, format="PNG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {gen_time:.2f}ç§’")
        
        return jsonify({
            "image": img_b64,
            "width": width,
            "height": height,
            "seed": seed,
            "time": round(gen_time, 2),
        })
        
    except torch.cuda.OutOfMemoryError:
        print("   âŒ CUDA å†…å­˜ä¸è¶³!")
        torch.cuda.empty_cache()
        return jsonify({"error": "GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¾ƒå°çš„å°ºå¯¸"}), 507
        
    except Exception as e:
        print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route("/info", methods=["GET"])
def info():
    """æ¨¡å‹ä¿¡æ¯"""
    return jsonify({
        "model": "Z-Image-Turbo",
        "developer": "Tongyi-MAI (é˜¿é‡Œå·´å·´é€šä¹‰)",
        "parameters": "6B",
        "recommended_steps": 9,
        "supported_resolutions": [512, 768, 1024, 1280, 1536, 2048],
        "features": [
            "ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ",
            "ä¼˜ç§€çš„æ–‡å­—æ¸²æŸ“",
            "ç…§ç‰‡çº§çœŸå®æ„Ÿ",
            "8æ­¥å¿«é€Ÿæ¨ç†",
        ],
        "gpu": {
            "name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "vram_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1) if torch.cuda.is_available() else None,
        }
    })


if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    load_model()
    
    # å¯åŠ¨æœåŠ¡
    # æ³¨æ„: ä½¿ç”¨ threaded=False é¿å…å¤šçº¿ç¨‹è®¿é—® CUDA
    app.run(
        host="0.0.0.0",
        port=8199,
        threaded=False,
        debug=False
    )
