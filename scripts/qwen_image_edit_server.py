#!/usr/bin/env python3
"""
Qwen-Image-Edit æœ¬åœ°æ¨ç†æœåŠ¡
æä¾› REST API ç”¨äºå›¾åƒç¼–è¾‘

æ¨¡å‹: Qwen/Qwen-Image-Edit (20Bå‚æ•°)
- æ”¯æŒè¯­ä¹‰ç¼–è¾‘å’Œå¤–è§‚ç¼–è¾‘
- ç²¾ç¡®çš„æ–‡å­—ç¼–è¾‘èƒ½åŠ›
- ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ

API ç«¯ç‚¹:
- GET  /health - å¥åº·æ£€æŸ¥
- POST /edit   - å›¾åƒç¼–è¾‘
- GET  /info   - æ¨¡å‹ä¿¡æ¯
"""

import os
import sys
import time
import base64
from io import BytesIO
from datetime import datetime

import torch
from flask import Flask, request, jsonify

app = Flask(__name__)

# å…¨å±€å˜é‡
pipe = None
model_loaded = False
USE_QUANTIZATION = os.environ.get("USE_QUANTIZATION", "true").lower() == "true"


def load_model():
    """åŠ è½½ Qwen-Image-Edit æ¨¡å‹"""
    global pipe, model_loaded
    
    print("\n" + "=" * 60)
    print("ğŸš€ Qwen-Image-Edit æœ¬åœ°æ¨ç†æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ PyTorch: {torch.__version__}")
    print(f"ğŸ® CUDA: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    print(f"\nğŸ“¦ æ­£åœ¨åŠ è½½ Qwen-Image-Edit æ¨¡å‹...")
    print(f"   æ¥æº: Qwen/Qwen-Image-Edit (HuggingFace)")
    print(f"   å‚æ•°: 20B")
    print(f"   é‡åŒ–: {'8-bit (bitsandbytes)' if USE_QUANTIZATION else 'åŸå§‹ç²¾åº¦'}")
    
    start_time = time.time()
    
    try:
        from diffusers import QwenImageEditPipeline
        
        # æ¨¡å‹åŠ è½½é…ç½®
        model_id = "Qwen/Qwen-Image-Edit"
        
        if USE_QUANTIZATION:
            # ä½¿ç”¨ 8-bit é‡åŒ–å‡å°‘æ˜¾å­˜
            try:
                # å°è¯•ä» diffusers ä¸åŒè·¯å¾„å¯¼å…¥é‡åŒ–é…ç½®
                quantization_config_cls = None
                try:
                    from diffusers import BitsAndBytesConfig
                    quantization_config_cls = BitsAndBytesConfig
                except ImportError:
                    try:
                        from diffusers.utils import BitsAndBytesConfig
                        quantization_config_cls = BitsAndBytesConfig
                    except ImportError:
                        try:
                            from diffusers.quantizers import BitsAndBytesConfig
                            quantization_config_cls = BitsAndBytesConfig
                        except ImportError:
                            pass
                
                if quantization_config_cls:
                    print(f"   ğŸ“¦ ä½¿ç”¨é‡åŒ–é…ç½®ç±»: {quantization_config_cls.__module__}.{quantization_config_cls.__name__}")
                    quantization_config = quantization_config_cls(
                        load_in_8bit=True,
                        bnb_8bit_compute_dtype=torch.bfloat16,
                    )
                    
                    pipe = QwenImageEditPipeline.from_pretrained(
                        model_id,
                        torch_dtype=torch.bfloat16,
                        quantization_config=quantization_config,
                        low_cpu_mem_usage=True,
                    )
                    print("   âœ… 8-bit é‡åŒ–æ¨¡å¼å·²å¯ç”¨")
                else:
                    raise ImportError("æ— æ³•æ‰¾åˆ° diffusers.BitsAndBytesConfig")
                
            except Exception as e:
                print(f"   âš ï¸ é‡åŒ–åŠ è½½å¤±è´¥: {e}")
                print("   ğŸ”„ å°è¯• transformers é‡åŒ–é…ç½®å›é€€...")
                
                # å›é€€æ–¹æ¡ˆï¼šåˆ†åˆ«åŠ è½½ç»„ä»¶
                # Qwen-Image-Edit = Tokenizer + Text Encoder (Qwen2-VL) + VAE + Transformer + Scheduler
                try:
                    from transformers import BitsAndBytesConfig as TrBitsAndBytesConfig
                    from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer
                    from diffusers import AutoencoderKL, QwenImageEditPipeline
                    
                    print("   ğŸ“¦ æ‰‹åŠ¨åŠ è½½å¹¶é‡åŒ–ç»„ä»¶...")
                    
                    # 1. é‡åŒ– Text Encoder (è¿™æ˜¯æ˜¾å­˜å¤§æˆ·)
                    bnb_config = TrBitsAndBytesConfig(
                        load_in_8bit=True,
                        bnb_8bit_compute_dtype=torch.bfloat16
                    )
                    
                    # æ³¨æ„: Qwen-Image-Edit å¯èƒ½ä½¿ç”¨ç‰¹å®šçš„å­æ–‡ä»¶å¤¹
                    # æˆ‘ä»¬å°è¯•ä» pipeline é…ç½®ä¸­æ¨æ–­æˆ–è€…ç›´æ¥åŠ è½½
                    # è¿™é‡Œä¸ºäº†ä¿é™©ï¼Œè¿˜æ˜¯å°è¯•ç›´æ¥å›é€€åˆ°æ ‡å‡†æ¨¡å¼ä½†å¼€å¯æ›´æ¿€è¿›çš„ offload
                    # å› ä¸ºæ‰‹åŠ¨æ‹¼è£… pipeline é£é™©å¾ˆå¤§ï¼Œä¸”å®¹æ˜“å‡ºé”™
                    print("   âš ï¸ ç»„ä»¶åˆ†ç¦»åŠ è½½è¿‡äºå¤æ‚ï¼Œè½¬ä¸ºæ ‡å‡†æ¨¡å¼ + CPU Offload")
                    raise e
                    
                except Exception as e2:
                    print(f"   âš ï¸ æœ€ç»ˆé‡åŒ–å¤±è´¥: {e2}")
                    print("   ğŸ”„ å›é€€åˆ°æ ‡å‡†æ¨¡å¼ (Sequential CPU Offload)...")
                    
                    # å°è¯•åŠ è½½ pipeline (ä¸å¸¦é‡åŒ–)ï¼Œä½†å¼€å¯ low_cpu_mem_usage
                    # å¦‚æœåªæœ‰ 16GB VRAMï¼Œå…¨é‡åŠ è½½å¯èƒ½ä¼šåœ¨ to("cuda") æ—¶å¤±è´¥
                    # æ‰€ä»¥æˆ‘ä»¬å…ˆåŠ è½½åˆ° CPUï¼Œç„¶å enable_sequential_cpu_offload
                    pipe = QwenImageEditPipeline.from_pretrained(
                        model_id,
                        torch_dtype=torch.bfloat16,
                        low_cpu_mem_usage=True,
                        device_map="balance" # å°è¯•è®© accelerate è‡ªåŠ¨åˆ†é…
                    )
                    # ä¸è¦è°ƒç”¨ pipe.to("cuda")ï¼Œè€Œæ˜¯ä½¿ç”¨ offload
                    pass
        else:
            # æ ‡å‡†åŠ è½½
            pipe = QwenImageEditPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
            )
            pipe.to("cuda")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ CPU offload
        if torch.cuda.is_available():
            total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if total_vram < 20: # 20GB ä»¥ä¸‹éƒ½å»ºè®®å¼€å¯ offloadï¼Œé™¤éå·²ç»é‡åŒ–ä¸”å¾ˆå°
                # æ£€æŸ¥æ˜¯å¦å·²é‡åŒ–
                is_quantized = hasattr(pipe, "quantization_config") and pipe.quantization_config is not None
                
                if not is_quantized:
                    print(f"   âš ï¸ GPU æ˜¾å­˜ ({total_vram:.1f}GB) å¯èƒ½ä¸è¶³ä»¥è¿è¡Œ 20B æ¨¡å‹(éé‡åŒ–)")
                    print("   ğŸ”„ å¯ç”¨ Sequential CPU Offload (é€Ÿåº¦è¾ƒæ…¢ä½†çœæ˜¾å­˜)...")
                    pipe.enable_sequential_cpu_offload()
                elif total_vram < 10: # å³ä¾¿æ˜¯ 8-bitï¼Œå¦‚æœæ˜¾å­˜å°äº 10GB ä¹Ÿè¦å°å¿ƒ
                    print(f"   âš ï¸ æ˜¾å­˜ç´§å¼ ï¼Œå¯ç”¨ Model CPU Offload...")
                    pipe.enable_model_cpu_offload()
        
        pipe.set_progress_bar_config(disable=True)

        
        load_time = time.time() - start_time
        model_loaded = True
        
        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.1f}ç§’")
        print("ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:8200")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "ok" if model_loaded else "loading",
        "model": "Qwen-Image-Edit",
        "cuda": torch.cuda.is_available(),
        "quantized": USE_QUANTIZATION,
    })


@app.route("/info", methods=["GET"])
def info():
    """æ¨¡å‹ä¿¡æ¯"""
    return jsonify({
        "model": "Qwen-Image-Edit",
        "developer": "Alibaba Qwen (é˜¿é‡Œå·´å·´é€šä¹‰)",
        "parameters": "20B",
        "quantization": "8-bit" if USE_QUANTIZATION else "bfloat16",
        "features": [
            "è¯­ä¹‰ç¼–è¾‘ (å¯¹è±¡æ—‹è½¬ã€é£æ ¼è½¬æ¢)",
            "å¤–è§‚ç¼–è¾‘ (æ·»åŠ /åˆ é™¤/ä¿®æ”¹å…ƒç´ )",
            "ç²¾ç¡®æ–‡å­—ç¼–è¾‘",
            "ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ",
        ],
        "endpoints": [
            {"method": "GET", "path": "/health", "description": "å¥åº·æ£€æŸ¥"},
            {"method": "POST", "path": "/edit", "description": "å›¾åƒç¼–è¾‘"},
            {"method": "GET", "path": "/info", "description": "æ¨¡å‹ä¿¡æ¯"},
        ],
        "gpu": {
            "name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "vram_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1) if torch.cuda.is_available() else None,
        }
    })


@app.route("/edit", methods=["POST"])
def edit():
    """
    å›¾åƒç¼–è¾‘
    
    POST JSON:
    {
        "prompt": "ç¼–è¾‘æŒ‡ä»¤ (æ”¯æŒä¸­è‹±æ–‡)",
        "image": "base64ç¼–ç çš„è¾“å…¥å›¾åƒ",
        "cfg_scale": 4.0,        // å¯é€‰ï¼Œé»˜è®¤ 4.0
        "steps": 50,             // å¯é€‰ï¼Œé»˜è®¤ 50
        "seed": 42               // å¯é€‰ï¼Œéšæœºç§å­
    }
    
    è¿”å›:
    {
        "image": "base64ç¼–ç çš„PNGå›¾åƒ",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "time": 5.23
    }
    """
    if not model_loaded:
        return jsonify({"error": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•"}), 503
    
    try:
        from PIL import Image
        
        data = request.json or {}
        
        prompt = data.get("prompt", "")
        image_b64 = data.get("image", "")
        cfg_scale = float(data.get("cfg_scale", 4.0))
        steps = int(data.get("steps", 50))
        seed = data.get("seed", None)
        negative_prompt = data.get("negative_prompt", " ")
        
        if not prompt:
            return jsonify({"error": "prompt å‚æ•°æ˜¯å¿…éœ€çš„"}), 400
        
        if not image_b64:
            return jsonify({"error": "image å‚æ•°æ˜¯å¿…éœ€çš„ (base64ç¼–ç )"}), 400
        
        # è§£ç è¾“å…¥å›¾åƒ
        try:
            if "base64," in image_b64:
                image_b64 = image_b64.split("base64,")[1]
            image_data = base64.b64decode(image_b64)
            input_image = Image.open(BytesIO(image_data)).convert("RGB")
        except Exception as e:
            return jsonify({"error": f"å›¾åƒè§£ç å¤±è´¥: {e}"}), 400
        
        # ç”Ÿæˆå™¨ (ç§å­)
        if seed is None:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.manual_seed(seed)
        
        width, height = input_image.size
        
        print(f"\nğŸ¨ [{datetime.now().strftime('%H:%M:%S')}] å›¾åƒç¼–è¾‘è¯·æ±‚")
        print(f"   Prompt: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
        print(f"   å°ºå¯¸: {width}x{height}, CFG: {cfg_scale}, æ­¥æ•°: {steps}, ç§å­: {seed}")
        
        start_time = time.time()
        
        # æ‰§è¡Œç¼–è¾‘
        with torch.inference_mode():
            output = pipe(
                image=input_image,
                prompt=prompt,
                negative_prompt=negative_prompt,
                generator=generator,
                true_cfg_scale=cfg_scale,
                num_inference_steps=steps,
            )
        
        output_image = output.images[0]
        gen_time = time.time() - start_time
        
        # è½¬ base64
        buffer = BytesIO()
        output_image.save(buffer, format="PNG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {gen_time:.2f}ç§’")
        
        return jsonify({
            "image": img_b64,
            "width": output_image.width,
            "height": output_image.height,
            "seed": seed,
            "time": round(gen_time, 2),
        })
        
    except torch.cuda.OutOfMemoryError:
        print("   âŒ CUDA å†…å­˜ä¸è¶³!")
        torch.cuda.empty_cache()
        return jsonify({"error": "GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¾ƒå°çš„å›¾åƒ"}), 507
        
    except Exception as e:
        print(f"   âŒ ç¼–è¾‘å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    load_model()
    
    # å¯åŠ¨ Flask æœåŠ¡
    app.run(host="0.0.0.0", port=8200, threaded=False)
