#!/usr/bin/env python3
"""
Qwen-Image-Edit æœ¬åœ°æ¨ç†æœåŠ¡
æä¾› REST API ç”¨äºå›¾åƒç¼–è¾‘

æ¨¡å‹: Qwen/Qwen-Image-Edit (20Bå‚æ•°)
- æ”¯æŒè¯­ä¹‰ç¼–è¾‘å’Œå¤–è§‚ç¼–è¾‘
- ç²¾ç¡®çš„æ–‡å­—ç¼–è¾‘èƒ½åŠ›
- ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ

API ç«¯ç‚¹:
- GET  /health - å¥åº·æ£€æŸ¥
- POST /edit   - å›¾åƒç¼–è¾‘
- GET  /info   - æ¨¡å‹ä¿¡æ¯

é‡åŒ–è¯´æ˜:
- diffusers çš„ BitsAndBytesConfig åªèƒ½ç”¨äºå•ä¸ªæ¨¡å‹ç»„ä»¶(å¦‚ transformer)
- ä¸èƒ½ç›´æ¥å¯¹æ•´ä¸ª Pipeline ä½¿ç”¨é‡åŒ–é…ç½®
- æ­£ç¡®åšæ³•ï¼šåˆ†åˆ«åŠ è½½å¹¶é‡åŒ– transformer å’Œ text_encoderï¼Œå†ç»„è£… Pipeline
"""

import os
import sys
import time
import base64
from io import BytesIO
from datetime import datetime

import torch
from flask import Flask, request, jsonify

app = Flask(__name__)

# å…¨å±€å˜é‡
pipe = None
model_loaded = False
quantization_mode = "none"  # "8bit", "4bit", "none"
USE_QUANTIZATION = os.environ.get("USE_QUANTIZATION", "true").lower() == "true"
QUANTIZATION_BITS = os.environ.get("QUANTIZATION_BITS", "8")  # "8" æˆ– "4"


def load_model():
    """åŠ è½½ Qwen-Image-Edit æ¨¡å‹"""
    global pipe, model_loaded, quantization_mode
    
    print("\n" + "=" * 60)
    print("ğŸš€ Qwen-Image-Edit æœ¬åœ°æ¨ç†æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ PyTorch: {torch.__version__}")
    print(f"ğŸ® CUDA: {torch.cuda.is_available()}")
    
    total_vram = 0
    if torch.cuda.is_available():
        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {total_vram:.1f} GB")
    
    print(f"\nğŸ“¦ æ­£åœ¨åŠ è½½ Qwen-Image-Edit æ¨¡å‹...")
    print(f"   æ¥æº: Qwen/Qwen-Image-Edit (HuggingFace)")
    print(f"   å‚æ•°: 20B")
    print(f"   é‡åŒ–: {'å¯ç”¨ (' + QUANTIZATION_BITS + '-bit)' if USE_QUANTIZATION else 'ç¦ç”¨'}")
    
    start_time = time.time()
    
    try:
        from diffusers import QwenImageEditPipeline
        
        model_id = "Qwen/Qwen-Image-Edit"
        
        # ====================================================================
        # åŠ è½½ç­–ç•¥è¯´æ˜:
        # - é‡åŒ–æ¨¡å¼: 4-bit/8-bit é‡åŒ–ï¼Œç›´æ¥æ”¾ GPUï¼Œä¸ä½¿ç”¨ CPU offload
        #   (é‡åŒ–æ¨¡å‹ä¸ CPU offload ä¸å…¼å®¹ï¼Œä¼šæŠ¥ meta tensor é”™è¯¯)
        # - éé‡åŒ–æ¨¡å¼: ä½¿ç”¨ CPU offload èŠ‚çœæ˜¾å­˜ï¼Œä½†æ¨ç†è¾ƒæ…¢
        # ====================================================================
        
        if USE_QUANTIZATION:
            # ============================================================
            # æ··åˆé‡åŒ–æ¨¡å¼ï¼šTransformer é‡åŒ–æ”¾ GPUï¼ŒText_Encoder æ”¾ CPU
            # è¿™æ˜¯ 16GB æ˜¾å¡çš„å”¯ä¸€å¯è¡Œæ–¹æ¡ˆï¼
            # 20B æ¨¡å‹å³ä½¿å…¨éƒ¨ 4-bit é‡åŒ–ä¹Ÿéœ€è¦ ~12GBï¼ŒåŠ ä¸Šæ¨ç†æ¿€æ´»å€¼ä¼š OOM
            # ============================================================
            print("\n   ğŸ“¦ ä½¿ç”¨æ··åˆé‡åŒ–æ¨¡å¼...")
            print("      (Transformer-GPUé‡åŒ– + TextEncoder-CPU)")
            
            try:
                from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig
                from diffusers import AutoModel
                # ä½¿ç”¨æ­£ç¡®çš„ text_encoder ç±»å‹
                from transformers import Qwen2_5_VLForConditionalGeneration
                
                use_4bit = QUANTIZATION_BITS == "4"
                
                if use_4bit:
                    print("   ğŸ”§ Transformer: 4-bit NF4 é‡åŒ–")
                    diffusers_quant_config = DiffusersBitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_use_double_quant=True,
                    )
                    quantization_mode = "4bit-hybrid"
                else:
                    print("   ğŸ”§ Transformer: 8-bit é‡åŒ–")
                    diffusers_quant_config = DiffusersBitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    quantization_mode = "8bit-hybrid"
                
                # 1. é‡åŒ–åŠ è½½ transformer â†’ GPU
                print("   ğŸ“¦ [1/3] åŠ è½½ transformer (é‡åŒ– â†’ GPU)...")
                transformer_quantized = AutoModel.from_pretrained(
                    model_id,
                    subfolder="transformer",
                    quantization_config=diffusers_quant_config,
                    torch_dtype=torch.bfloat16,
                )
                print(f"      âœ… Transformer å·²åŠ è½½ (GPU, {quantization_mode})")
                
                # æ¸…ç†æ˜¾å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    import gc
                    gc.collect()
                
                # 2. text_encoder æ”¾ CPU (ä¸é‡åŒ–)
                # 16GB æ˜¾å¡æ— æ³•åŒæ—¶åœ¨ GPU æ”¾ transformer + text_encoder
                print("   ğŸ“¦ [2/3] åŠ è½½ text_encoder (CPU, bfloat16)...")
                text_encoder_cpu = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    model_id,
                    subfolder="text_encoder",
                    torch_dtype=torch.bfloat16,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                )
                print(f"      âœ… Text Encoder å·²åŠ è½½ (CPU)")
                
                # 3. ç»„è£… Pipeline
                print("   ğŸ“¦ [3/3] ç»„è£… Pipeline...")
                pipe = QwenImageEditPipeline.from_pretrained(
                    model_id,
                    transformer=transformer_quantized,
                    text_encoder=text_encoder_cpu,
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                )
                
                # VAE ç§»åˆ° GPU (è¾ƒå°ï¼Œçº¦ 300MB)
                if hasattr(pipe, 'vae') and pipe.vae is not None:
                    pipe.vae = pipe.vae.to(dtype=torch.float16, device="cuda")
                
                # å¯ç”¨æ˜¾å­˜ä¼˜åŒ–
                try:
                    pipe.enable_xformers_memory_efficient_attention()
                    print("   âœ… xFormers å·²å¯ç”¨")
                except Exception:
                    pass
                
                try:
                    if hasattr(pipe, 'enable_vae_slicing'):
                        pipe.enable_vae_slicing()
                    if hasattr(pipe, 'enable_vae_tiling'):
                        pipe.enable_vae_tiling()
                except Exception:
                    pass
                
                print(f"\n   âœ… æ··åˆæ¨¡å¼å°±ç»ª!")
                print(f"      Transformer: GPU (é‡åŒ–)")
                print(f"      TextEncoder: CPU (æ¨ç†æ—¶ä¼šè¾ƒæ…¢)")
                print(f"      VAE: GPU (fp16)")
                
            except Exception as e:
                print(f"   âš ï¸ é‡åŒ–åŠ è½½å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
                # å›é€€åˆ°éé‡åŒ– + CPU Offload æ¨¡å¼
                print("   ğŸ”„ å›é€€åˆ°éé‡åŒ– + CPU Offload æ¨¡å¼...")
                quantization_mode = "none"
                
                pipe = QwenImageEditPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                )
                print("   ğŸ”„ å¯ç”¨ Sequential CPU Offload (æ…¢ä½†ç¨³å®š)...")
                pipe.enable_sequential_cpu_offload()
        else:
            # ============================================================
            # éé‡åŒ–æ¨¡å¼: ä½¿ç”¨ CPU Offload èŠ‚çœæ˜¾å­˜
            # ============================================================
            print("   ğŸ“¦ éé‡åŒ–æ¨¡å¼åŠ è½½...")
            pipe = QwenImageEditPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
            )
            
            if total_vram < 40:
                print(f"   âš ï¸ GPU æ˜¾å­˜ ({total_vram:.1f}GB) ä¸è¶³å®Œå…¨åŠ è½½ 20B æ¨¡å‹")
                print("   ğŸ”„ å¯ç”¨ Sequential CPU Offload...")
                pipe.enable_sequential_cpu_offload()
            else:
                pipe.to("cuda")
        
        pipe.set_progress_bar_config(disable=True)

        
        load_time = time.time() - start_time
        model_loaded = True
        
        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.1f}ç§’")
        print("ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:8200")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "ok" if model_loaded else "loading",
        "model": "Qwen-Image-Edit",
        "cuda": torch.cuda.is_available(),
        "quantization": quantization_mode,
    })


@app.route("/info", methods=["GET"])
def info():
    """æ¨¡å‹ä¿¡æ¯"""
    quant_desc = {
        "8bit-hybrid": "8-bit æ··åˆ (Transformer-GPU + TextEncoder-CPU)",
        "4bit-hybrid": "4-bit æ··åˆ (Transformer-GPU + TextEncoder-CPU)",
        "8bit": "8-bit å…¨é‡åŒ– (éœ€24GB+æ˜¾å­˜)",
        "4bit": "4-bit å…¨é‡åŒ– (éœ€20GB+æ˜¾å­˜)",
        "none": "bfloat16 + CPU Offload"
    }.get(quantization_mode, quantization_mode)
    
    return jsonify({
        "model": "Qwen-Image-Edit",
        "developer": "Alibaba Qwen (é˜¿é‡Œå·´å·´é€šä¹‰)",
        "parameters": "20B",
        "quantization": quant_desc,
        "quantization_mode": quantization_mode,
        "features": [
            "è¯­ä¹‰ç¼–è¾‘ (å¯¹è±¡æ—‹è½¬ã€é£æ ¼è½¬æ¢)",
            "å¤–è§‚ç¼–è¾‘ (æ·»åŠ /åˆ é™¤/ä¿®æ”¹å…ƒç´ )",
            "ç²¾ç¡®æ–‡å­—ç¼–è¾‘",
            "ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ",
        ],
        "endpoints": [
            {"method": "GET", "path": "/health", "description": "å¥åº·æ£€æŸ¥"},
            {"method": "POST", "path": "/edit", "description": "å›¾åƒç¼–è¾‘"},
            {"method": "GET", "path": "/info", "description": "æ¨¡å‹ä¿¡æ¯"},
        ],
        "gpu": {
            "name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "vram_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1) if torch.cuda.is_available() else None,
        },
        "limits": {
            "default_max_size": 768,
            "default_steps": 28,
            "recommended": {
                "16GB_hybrid": {"max_size": 768, "steps": 28, "note": "Transformer-GPU + TextEncoder-CPU"},
                "24GB_4bit": {"max_size": 1024, "steps": 50},
                "48GB": {"max_size": 1536, "steps": 50},
            },
            "memory_breakdown": {
                "transformer_4bit": "~8GB",
                "text_encoder_cpu": "~10GB RAM",
                "vae_fp16": "~0.3GB",
                "inference_activation": "~4-6GB (è§†åˆ†è¾¨ç‡)"
            }
        }
    })


@app.route("/edit", methods=["POST"])
def edit():
    """
    å›¾åƒç¼–è¾‘
    
    POST JSON:
    {
        "prompt": "ç¼–è¾‘æŒ‡ä»¤ (æ”¯æŒä¸­è‹±æ–‡)",
        "image": "base64ç¼–ç çš„è¾“å…¥å›¾åƒ",
        "cfg_scale": 4.0,        // å¯é€‰ï¼Œé»˜è®¤ 4.0
        "steps": 28,             // å¯é€‰ï¼Œé»˜è®¤ 28 (å®˜æ–¹æ¨è 28-50)
        "seed": 42,              // å¯é€‰ï¼Œéšæœºç§å­
        "max_size": 1024         // å¯é€‰ï¼Œæœ€å¤§å›¾åƒå°ºå¯¸ (é»˜è®¤1024ï¼Œ16GB+4bitå¯ç”¨)
    }
    
    è¿”å›:
    {
        "image": "base64ç¼–ç çš„PNGå›¾åƒ",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "time": 5.23
    }
    """
    if not model_loaded:
        return jsonify({"error": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•"}), 503
    
    try:
        from PIL import Image
        
        data = request.json or {}
        
        prompt = data.get("prompt", "")
        image_b64 = data.get("image", "")
        cfg_scale = float(data.get("cfg_scale", 4.0))
        # é»˜è®¤ 28 æ­¥ï¼ŒQwen-Image-Edit å®˜æ–¹æ¨è 28-50 æ­¥
        steps = int(data.get("steps", 28))
        seed = data.get("seed", None)
        negative_prompt = data.get("negative_prompt", " ")
        # æœ€å¤§å›¾åƒå°ºå¯¸ - 16GBæ˜¾å­˜+4bitå»ºè®®768ï¼Œ24GBå¯ç”¨1024
        max_size = int(data.get("max_size", 768))
        
        if not prompt:
            return jsonify({"error": "prompt å‚æ•°æ˜¯å¿…éœ€çš„"}), 400
        
        if not image_b64:
            return jsonify({"error": "image å‚æ•°æ˜¯å¿…éœ€çš„ (base64ç¼–ç )"}), 400
        
        # è§£ç è¾“å…¥å›¾åƒ
        try:
            if "base64," in image_b64:
                image_b64 = image_b64.split("base64,")[1]
            image_data = base64.b64decode(image_b64)
            input_image = Image.open(BytesIO(image_data)).convert("RGB")
        except Exception as e:
            return jsonify({"error": f"å›¾åƒè§£ç å¤±è´¥: {e}"}), 400
        
        # è®°å½•åŸå§‹å°ºå¯¸
        original_width, original_height = input_image.size
        
        # ============================================================
        # å›¾åƒå°ºå¯¸é™åˆ¶ - é˜²æ­¢æ˜¾å­˜æº¢å‡º
        # 16GB æ˜¾å­˜ + 4-bit é‡åŒ–ï¼šå»ºè®®æœ€å¤§ 1024x1024
        # ============================================================
        if max(original_width, original_height) > max_size:
            # æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼Œä¿æŒé•¿è¾¹ä¸è¶…è¿‡ max_size
            if original_width > original_height:
                new_width = max_size
                new_height = int(original_height * max_size / original_width)
            else:
                new_height = max_size
                new_width = int(original_width * max_size / original_height)
            
            # ç¡®ä¿å°ºå¯¸æ˜¯ 8 çš„å€æ•° (æŸäº›æ¨¡å‹è¦æ±‚)
            new_width = (new_width // 8) * 8
            new_height = (new_height // 8) * 8
            
            input_image = input_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(f"   ğŸ“ å›¾åƒç¼©æ”¾: {original_width}x{original_height} â†’ {new_width}x{new_height}")
        
        # ç”Ÿæˆå™¨ (ç§å­)
        if seed is None:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.manual_seed(seed)
        
        width, height = input_image.size
        
        print(f"\nğŸ¨ [{datetime.now().strftime('%H:%M:%S')}] å›¾åƒç¼–è¾‘è¯·æ±‚")
        print(f"   Prompt: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
        print(f"   åŸå§‹å°ºå¯¸: {original_width}x{original_height}, å¤„ç†å°ºå¯¸: {width}x{height}")
        print(f"   CFG: {cfg_scale}, æ­¥æ•°: {steps}, ç§å­: {seed}")
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            import gc
            gc.collect()
        
        start_time = time.time()
        
        # æ‰§è¡Œç¼–è¾‘
        # ä½¿ç”¨ torch.cuda.amp.autocast è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
        with torch.inference_mode():
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                output = pipe(
                    image=input_image,
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    generator=generator,
                    true_cfg_scale=cfg_scale,
                    num_inference_steps=steps,
                )
        
        output_image = output.images[0]
        gen_time = time.time() - start_time
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è½¬ base64
        buffer = BytesIO()
        output_image.save(buffer, format="PNG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {gen_time:.2f}ç§’")
        
        return jsonify({
            "image": img_b64,
            "width": output_image.width,
            "height": output_image.height,
            "original_width": original_width,
            "original_height": original_height,
            "seed": seed,
            "time": round(gen_time, 2),
        })
        
    except torch.cuda.OutOfMemoryError:
        print("   âŒ CUDA å†…å­˜ä¸è¶³!")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return jsonify({
            "error": "GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¾ƒå°çš„å›¾åƒæˆ–é™ä½ max_size å‚æ•° (é»˜è®¤ 1024)",
            "hint": "å¯ä»¥åœ¨è¯·æ±‚ä¸­æ·»åŠ  'max_size': 768 æˆ–æ›´å°çš„å€¼"
        }), 507
        
    except RuntimeError as e:
        error_msg = str(e)
        print(f"   âŒ è¿è¡Œæ—¶é”™è¯¯: {error_msg}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        if "out of memory" in error_msg.lower():
            return jsonify({
                "error": "GPU å†…å­˜ä¸è¶³",
                "hint": "è¯·åœ¨è¯·æ±‚ä¸­æ·»åŠ  'max_size': 768 æˆ–æ›´å°çš„å€¼æ¥é™åˆ¶å›¾åƒå°ºå¯¸"
            }), 507
        import traceback
        traceback.print_exc()
        return jsonify({"error": error_msg}), 500
        
    except Exception as e:
        print(f"   âŒ ç¼–è¾‘å¤±è´¥: {e}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    load_model()
    
    # å¯åŠ¨ Flask æœåŠ¡
    app.run(host="0.0.0.0", port=8200, threaded=False)
