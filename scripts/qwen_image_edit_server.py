#!/usr/bin/env python3
"""
Qwen-Image-Edit æœ¬åœ°æ¨ç†æœåŠ¡
æä¾› REST API ç”¨äºå›¾åƒç¼–è¾‘

æ¨¡å‹: Qwen/Qwen-Image-Edit (20Bå‚æ•°)
- æ”¯æŒè¯­ä¹‰ç¼–è¾‘å’Œå¤–è§‚ç¼–è¾‘
- ç²¾ç¡®çš„æ–‡å­—ç¼–è¾‘èƒ½åŠ›
- ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ

API ç«¯ç‚¹:
- GET  /health - å¥åº·æ£€æŸ¥
- POST /edit   - å›¾åƒç¼–è¾‘
- GET  /info   - æ¨¡å‹ä¿¡æ¯

é‡åŒ–è¯´æ˜:
- diffusers çš„ BitsAndBytesConfig åªèƒ½ç”¨äºå•ä¸ªæ¨¡å‹ç»„ä»¶(å¦‚ transformer)
- ä¸èƒ½ç›´æ¥å¯¹æ•´ä¸ª Pipeline ä½¿ç”¨é‡åŒ–é…ç½®
- æ­£ç¡®åšæ³•ï¼šåˆ†åˆ«åŠ è½½å¹¶é‡åŒ– transformer å’Œ text_encoderï¼Œå†ç»„è£… Pipeline
"""

import os
import sys
import time
import base64
from io import BytesIO
from datetime import datetime

import torch
from flask import Flask, request, jsonify

app = Flask(__name__)

# å…¨å±€å˜é‡
pipe = None
model_loaded = False
quantization_mode = "none"  # "8bit", "4bit", "none"
USE_QUANTIZATION = os.environ.get("USE_QUANTIZATION", "true").lower() == "true"
QUANTIZATION_BITS = os.environ.get("QUANTIZATION_BITS", "8")  # "8" æˆ– "4"


def load_model():
    """åŠ è½½ Qwen-Image-Edit æ¨¡å‹"""
    global pipe, model_loaded, quantization_mode
    
    print("\n" + "=" * 60)
    print("ğŸš€ Qwen-Image-Edit æœ¬åœ°æ¨ç†æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”§ PyTorch: {torch.__version__}")
    print(f"ğŸ® CUDA: {torch.cuda.is_available()}")
    
    total_vram = 0
    if torch.cuda.is_available():
        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {total_vram:.1f} GB")
    
    print(f"\nğŸ“¦ æ­£åœ¨åŠ è½½ Qwen-Image-Edit æ¨¡å‹...")
    print(f"   æ¥æº: Qwen/Qwen-Image-Edit (HuggingFace)")
    print(f"   å‚æ•°: 20B")
    print(f"   é‡åŒ–: {'å¯ç”¨ (' + QUANTIZATION_BITS + '-bit)' if USE_QUANTIZATION else 'ç¦ç”¨'}")
    
    start_time = time.time()
    
    try:
        from diffusers import QwenImageEditPipeline
        
        model_id = "Qwen/Qwen-Image-Edit"
        
        if USE_QUANTIZATION:
            # ============================================================
            # æ­£ç¡®çš„é‡åŒ–æ–¹å¼ï¼šåˆ†åˆ«åŠ è½½å¹¶é‡åŒ–å„ç»„ä»¶
            # diffusers çš„ BitsAndBytesConfig åªèƒ½ç”¨äºå•ä¸ªæ¨¡å‹ï¼Œä¸èƒ½ç”¨äº Pipeline
            # ============================================================
            print("\n   ğŸ“¦ ä½¿ç”¨ç»„ä»¶çº§é‡åŒ– (æ¨èæ–¹å¼)...")
            
            try:
                from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig
                from diffusers import AutoModel
                from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig
                from transformers import AutoModel as TFAutoModel
                
                use_4bit = QUANTIZATION_BITS == "4"
                
                if use_4bit:
                    print("   ğŸ”§ ä½¿ç”¨ 4-bit NF4 é‡åŒ–...")
                    # 4-bit é‡åŒ–é…ç½® (æ›´çœæ˜¾å­˜)
                    diffusers_quant_config = DiffusersBitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_use_double_quant=True,  # åµŒå¥—é‡åŒ–ï¼Œé¢å¤–èŠ‚çœ 0.4 bits/param
                    )
                    transformers_quant_config = TransformersBitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_use_double_quant=True,
                    )
                    quantization_mode = "4bit"
                else:
                    print("   ğŸ”§ ä½¿ç”¨ 8-bit LLM.int8() é‡åŒ–...")
                    # 8-bit é‡åŒ–é…ç½®
                    diffusers_quant_config = DiffusersBitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    transformers_quant_config = TransformersBitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    quantization_mode = "8bit"
                
                # 1. é‡åŒ–åŠ è½½ transformer (diffusers ç»„ä»¶)
                print("   ğŸ“¦ åŠ è½½ transformer (é‡åŒ–)...")
                transformer_quantized = AutoModel.from_pretrained(
                    model_id,
                    subfolder="transformer",
                    quantization_config=diffusers_quant_config,
                    torch_dtype=torch.bfloat16,
                )
                print(f"      âœ… Transformer å·²åŠ è½½ ({quantization_mode})")
                
                # 2. é‡åŒ–åŠ è½½ text_encoder (transformers ç»„ä»¶)
                print("   ğŸ“¦ åŠ è½½ text_encoder (é‡åŒ–)...")
                text_encoder_quantized = TFAutoModel.from_pretrained(
                    model_id,
                    subfolder="text_encoder",
                    quantization_config=transformers_quant_config,
                    torch_dtype=torch.bfloat16,
                )
                print(f"      âœ… Text Encoder å·²åŠ è½½ ({quantization_mode})")
                
                # 3. ç»„è£… Pipelineï¼Œä¼ å…¥é‡åŒ–åçš„ç»„ä»¶
                # ä¸ä½¿ç”¨ device_mapï¼Œè®©æˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶ offload
                print("   ğŸ“¦ ç»„è£… Pipeline...")
                pipe = QwenImageEditPipeline.from_pretrained(
                    model_id,
                    transformer=transformer_quantized,
                    text_encoder=text_encoder_quantized,
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                )
                
                # å³ä½¿é‡åŒ–åï¼Œ16GB æ˜¾å­˜ä»ç„¶ç´§å¼ ï¼Œå¯ç”¨ CPU offload
                if total_vram < 24:
                    print(f"   âš ï¸ æ˜¾å­˜ ({total_vram:.1f}GB) ç´§å¼ ï¼Œå¯ç”¨ Sequential CPU Offload...")
                    pipe.enable_sequential_cpu_offload()
                else:
                    pipe.to("cuda")
                
                print(f"   âœ… {quantization_mode} é‡åŒ–æ¨¡å¼å·²å¯ç”¨")
                
            except Exception as e:
                print(f"   âš ï¸ ç»„ä»¶çº§é‡åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print("   ğŸ”„ å›é€€åˆ°æ ‡å‡†æ¨¡å¼ + CPU Offload...")
                quantization_mode = "none"
                
                # å›é€€æ–¹æ¡ˆï¼šä¸ä½¿ç”¨é‡åŒ–ï¼Œä½†ç”¨ CPU offload èŠ‚çœæ˜¾å­˜
                pipe = QwenImageEditPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                )
                # æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹© offload ç­–ç•¥
                if total_vram < 24:
                    print(f"   âš ï¸ GPU æ˜¾å­˜ ({total_vram:.1f}GB) ä¸è¶³è¿è¡Œ 20B æ¨¡å‹")
                    print("   ğŸ”„ å¯ç”¨ Sequential CPU Offload...")
                    pipe.enable_sequential_cpu_offload()
                else:
                    pipe.to("cuda")
        else:
            # éé‡åŒ–æ¨¡å¼
            print("   ğŸ“¦ æ ‡å‡†æ¨¡å¼åŠ è½½...")
            pipe = QwenImageEditPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
            )
            
            if total_vram < 40:
                # 20B æ¨¡å‹éé‡åŒ–çº¦éœ€ 40GB VRAM
                print(f"   âš ï¸ GPU æ˜¾å­˜ ({total_vram:.1f}GB) å¯èƒ½ä¸è¶³")
                if total_vram < 24:
                    print("   ğŸ”„ å¯ç”¨ Sequential CPU Offload (æœ€çœæ˜¾å­˜ä½†æœ€æ…¢)...")
                    pipe.enable_sequential_cpu_offload()
                else:
                    print("   ğŸ”„ å¯ç”¨ Model CPU Offload...")
                    pipe.enable_model_cpu_offload()
            else:
                pipe.to("cuda")
        
        pipe.set_progress_bar_config(disable=True)

        
        load_time = time.time() - start_time
        model_loaded = True
        
        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.1f}ç§’")
        print("ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:8200")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "ok" if model_loaded else "loading",
        "model": "Qwen-Image-Edit",
        "cuda": torch.cuda.is_available(),
        "quantization": quantization_mode,
    })


@app.route("/info", methods=["GET"])
def info():
    """æ¨¡å‹ä¿¡æ¯"""
    quant_desc = {
        "8bit": "8-bit (LLM.int8())",
        "4bit": "4-bit (NF4 + Double Quant)",
        "none": "bfloat16 (æ— é‡åŒ–)"
    }.get(quantization_mode, "unknown")
    
    return jsonify({
        "model": "Qwen-Image-Edit",
        "developer": "Alibaba Qwen (é˜¿é‡Œå·´å·´é€šä¹‰)",
        "parameters": "20B",
        "quantization": quant_desc,
        "features": [
            "è¯­ä¹‰ç¼–è¾‘ (å¯¹è±¡æ—‹è½¬ã€é£æ ¼è½¬æ¢)",
            "å¤–è§‚ç¼–è¾‘ (æ·»åŠ /åˆ é™¤/ä¿®æ”¹å…ƒç´ )",
            "ç²¾ç¡®æ–‡å­—ç¼–è¾‘",
            "ä¸­è‹±æ–‡åŒè¯­æ”¯æŒ",
        ],
        "endpoints": [
            {"method": "GET", "path": "/health", "description": "å¥åº·æ£€æŸ¥"},
            {"method": "POST", "path": "/edit", "description": "å›¾åƒç¼–è¾‘"},
            {"method": "GET", "path": "/info", "description": "æ¨¡å‹ä¿¡æ¯"},
        ],
        "gpu": {
            "name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "vram_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1) if torch.cuda.is_available() else None,
        },
        "limits": {
            "default_max_size": 512,
            "default_steps": 28,
            "recommended": {
                "16GB_VRAM": {"max_size": 512, "steps": 28},
                "24GB_VRAM": {"max_size": 768, "steps": 50},
                "40GB_VRAM": {"max_size": 1024, "steps": 50},
            }
        }
    })


@app.route("/edit", methods=["POST"])
def edit():
    """
    å›¾åƒç¼–è¾‘
    
    POST JSON:
    {
        "prompt": "ç¼–è¾‘æŒ‡ä»¤ (æ”¯æŒä¸­è‹±æ–‡)",
        "image": "base64ç¼–ç çš„è¾“å…¥å›¾åƒ",
        "cfg_scale": 4.0,        // å¯é€‰ï¼Œé»˜è®¤ 4.0
        "steps": 50,             // å¯é€‰ï¼Œé»˜è®¤ 50
        "seed": 42,              // å¯é€‰ï¼Œéšæœºç§å­
        "max_size": 512          // å¯é€‰ï¼Œæœ€å¤§å›¾åƒå°ºå¯¸ (é˜²æ­¢OOMï¼Œ16GBæ˜¾å­˜å»ºè®®512)
    }
    
    è¿”å›:
    {
        "image": "base64ç¼–ç çš„PNGå›¾åƒ",
        "width": 512,
        "height": 512,
        "seed": 42,
        "time": 5.23
    }
    """
    if not model_loaded:
        return jsonify({"error": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åé‡è¯•"}), 503
    
    try:
        from PIL import Image
        
        data = request.json or {}
        
        prompt = data.get("prompt", "")
        image_b64 = data.get("image", "")
        cfg_scale = float(data.get("cfg_scale", 4.0))
        # é»˜è®¤ 28 æ­¥ï¼ŒQwen-Image-Edit å®˜æ–¹æ¨è 28-50 æ­¥
        steps = int(data.get("steps", 28))
        seed = data.get("seed", None)
        negative_prompt = data.get("negative_prompt", " ")
        # æœ€å¤§å›¾åƒå°ºå¯¸ (16GB æ˜¾å­˜ + 4-bit é‡åŒ–å»ºè®® 512ï¼Œ24GB+ å¯ç”¨ 768-1024)
        max_size = int(data.get("max_size", 512))
        
        if not prompt:
            return jsonify({"error": "prompt å‚æ•°æ˜¯å¿…éœ€çš„"}), 400
        
        if not image_b64:
            return jsonify({"error": "image å‚æ•°æ˜¯å¿…éœ€çš„ (base64ç¼–ç )"}), 400
        
        # è§£ç è¾“å…¥å›¾åƒ
        try:
            if "base64," in image_b64:
                image_b64 = image_b64.split("base64,")[1]
            image_data = base64.b64decode(image_b64)
            input_image = Image.open(BytesIO(image_data)).convert("RGB")
        except Exception as e:
            return jsonify({"error": f"å›¾åƒè§£ç å¤±è´¥: {e}"}), 400
        
        # è®°å½•åŸå§‹å°ºå¯¸
        original_width, original_height = input_image.size
        
        # ============================================================
        # å›¾åƒå°ºå¯¸é™åˆ¶ - é˜²æ­¢æ˜¾å­˜æº¢å‡º
        # 16GB æ˜¾å­˜ + 4-bit é‡åŒ–ï¼šå»ºè®®æœ€å¤§ 1024x1024
        # ============================================================
        if max(original_width, original_height) > max_size:
            # æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼Œä¿æŒé•¿è¾¹ä¸è¶…è¿‡ max_size
            if original_width > original_height:
                new_width = max_size
                new_height = int(original_height * max_size / original_width)
            else:
                new_height = max_size
                new_width = int(original_width * max_size / original_height)
            
            # ç¡®ä¿å°ºå¯¸æ˜¯ 8 çš„å€æ•° (æŸäº›æ¨¡å‹è¦æ±‚)
            new_width = (new_width // 8) * 8
            new_height = (new_height // 8) * 8
            
            input_image = input_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(f"   ğŸ“ å›¾åƒç¼©æ”¾: {original_width}x{original_height} â†’ {new_width}x{new_height}")
        
        # ç”Ÿæˆå™¨ (ç§å­)
        if seed is None:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.manual_seed(seed)
        
        width, height = input_image.size
        
        print(f"\nğŸ¨ [{datetime.now().strftime('%H:%M:%S')}] å›¾åƒç¼–è¾‘è¯·æ±‚")
        print(f"   Prompt: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
        print(f"   åŸå§‹å°ºå¯¸: {original_width}x{original_height}, å¤„ç†å°ºå¯¸: {width}x{height}")
        print(f"   CFG: {cfg_scale}, æ­¥æ•°: {steps}, ç§å­: {seed}")
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        start_time = time.time()
        
        # æ‰§è¡Œç¼–è¾‘
        with torch.inference_mode():
            output = pipe(
                image=input_image,
                prompt=prompt,
                negative_prompt=negative_prompt,
                generator=generator,
                true_cfg_scale=cfg_scale,
                num_inference_steps=steps,
            )
        
        output_image = output.images[0]
        gen_time = time.time() - start_time
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è½¬ base64
        buffer = BytesIO()
        output_image.save(buffer, format="PNG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {gen_time:.2f}ç§’")
        
        return jsonify({
            "image": img_b64,
            "width": output_image.width,
            "height": output_image.height,
            "original_width": original_width,
            "original_height": original_height,
            "seed": seed,
            "time": round(gen_time, 2),
        })
        
    except torch.cuda.OutOfMemoryError:
        print("   âŒ CUDA å†…å­˜ä¸è¶³!")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return jsonify({
            "error": "GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¾ƒå°çš„å›¾åƒæˆ–é™ä½ max_size å‚æ•° (é»˜è®¤ 1024)",
            "hint": "å¯ä»¥åœ¨è¯·æ±‚ä¸­æ·»åŠ  'max_size': 768 æˆ–æ›´å°çš„å€¼"
        }), 507
        
    except RuntimeError as e:
        error_msg = str(e)
        print(f"   âŒ è¿è¡Œæ—¶é”™è¯¯: {error_msg}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        if "out of memory" in error_msg.lower():
            return jsonify({
                "error": "GPU å†…å­˜ä¸è¶³",
                "hint": "è¯·åœ¨è¯·æ±‚ä¸­æ·»åŠ  'max_size': 768 æˆ–æ›´å°çš„å€¼æ¥é™åˆ¶å›¾åƒå°ºå¯¸"
            }), 507
        import traceback
        traceback.print_exc()
        return jsonify({"error": error_msg}), 500
        
    except Exception as e:
        print(f"   âŒ ç¼–è¾‘å¤±è´¥: {e}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    load_model()
    
    # å¯åŠ¨ Flask æœåŠ¡
    app.run(host="0.0.0.0", port=8200, threaded=False)
